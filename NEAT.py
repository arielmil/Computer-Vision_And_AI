import os
import kagglehub
import numpy as np
import pickle
from PIL import Image
from typing import Union, List, Tuple

from sklearn.model_selection import train_test_split

from tensorneat.problem.supervised import SupervisedFuncFit
from tensorneat.pipeline import Pipeline
from tensorneat.algorithm import NEAT
from tensorneat.genome import DefaultGenome, DefaultMutation, DefaultCrossover, DefaultDistance, DefaultConn
from tensorneat.common import ACT, AGG
from tensorneat.genome.gene.node.bias import BiasNode
from tensorneat.problem.func_fit import FuncFit

import jax
from jax import numpy as jnp, vmap, random
import numpy as np

class SupervisedFuncFit(FuncFit):
    def __init__(
        self,
        X: Union[List, Tuple, np.ndarray],
        y: Union[List, Tuple, np.ndarray],
        batch_size: int = 256,  # ğŸ”¹ Processamento em lotes para reduzir consumo de memÃ³ria
        *args,
        **kwargs,
    ):
        """
        Problema de aprendizado supervisionado para TensorNEAT.

        X: Features (inputs)
        y: Labels (outputs, one-hot encoded)
        batch_size: Tamanho do batch para avaliaÃ§Ã£o (evita estouro de memÃ³ria)
        """
        self.data_inputs = jnp.array(X, dtype=jnp.float32)
        self.data_outputs = jnp.array(y, dtype=jnp.float32)
        self.batch_size = batch_size

        super().__init__(*args, **kwargs)

    def evaluate(self, state, randkey, act_func, params):
        """
        Calcula a funÃ§Ã£o de fitness usando Cross-Entropy Loss em batches.
        
        ParÃ¢metros:
            - state: Estado atual do algoritmo (necessÃ¡rio para o TensorNEAT)
            - randkey: Chave aleatÃ³ria do JAX (necessÃ¡rio para inicializaÃ§Ãµes ou mutaÃ§Ãµes)
            - act_func: FunÃ§Ã£o de ativaÃ§Ã£o da rede neural
            - params: ParÃ¢metros da rede (pesos e conexÃµes)
        
        Retorna:
            - Fitness (negativo da perda mÃ©dia)
        """

        # Se state for None, definir para array vazio para evitar erros
        if state is None:
            state = jnp.array([])

        # Definindo funÃ§Ã£o auxiliar para extrair a saÃ­da final
        def get_final_output(raw_output):
            if isinstance(raw_output, (tuple, list)):
                # Se for uma tupla com 4 ou mais elementos, assumimos que o segundo Ã© a saÃ­da final
                if len(raw_output) >= 4:
                    return raw_output[1]
                else:
                    return raw_output[0]
            return raw_output

        num_samples = self.data_inputs.shape[0]
        num_batches = max(1, num_samples // self.batch_size)  # Evita divisÃ£o por zero
        total_loss = 0.0

        # Processamos os dados em batches
        for batch_idx in range(num_batches):
            batch_start = batch_idx * self.batch_size
            batch_end = min(batch_start + self.batch_size, num_samples)

            batch_X = self.data_inputs[batch_start:batch_end]
            batch_y = self.data_outputs[batch_start:batch_end]

            # Gerando variaÃ§Ã£o randÃ´mica baseada no randkey (pode ser usada para dropout ou noise nos dados)
            subkey, _ = jax.random.split(randkey)
            perturbed_X = batch_X + 0.01 * jax.random.normal(subkey, batch_X.shape)  # Pequena perturbaÃ§Ã£o (opcional)
            
            print(f"Printando antes do vmap e antes de perturbed_x passar por jnp.reshape:")
            print(f"ğŸš¨ Dentro de evaluate - batch_X.shape: {batch_X.shape}")
            print(f"ğŸš¨ Dentro de evaluate - esperado: ({self.batch_size}, 12600)")
            print(f"ğŸš¨ Dentro de evaluate - act_func antes: {act_func}")
            print(f"ğŸš¨ Dentro de evaluate - state.shape: {state.shape if isinstance(state, jnp.ndarray) else 'None'}")

            print(f"ğŸ“Œ Shape antes do vmap: {perturbed_X.shape}")
            print(f"Printando antes do vmap e depois de perturbed_x passar por jnp.reshape:")
            print(f"ğŸš¨ Dentro de evaluate - batch_X.shape: {batch_X.shape}")
            print(f"ğŸš¨ Dentro de evaluate - esperado: ({self.batch_size}, 12600)")
            print(f"ğŸš¨ Dentro de evaluate - act_func antes: {act_func}")
            print(f"ğŸš¨ Dentro de evaluate - state.shape: {state.shape if isinstance(state, jnp.ndarray) else 'None'}")
            
            print(f"ğŸ“Œ Antes de reshape - perturbed_X.shape: {perturbed_X.shape}")
            if len(perturbed_X.shape) > 2:
                perturbed_X = perturbed_X.reshape(perturbed_X.shape[0], -1)
            else:
                perturbed_X = perturbed_X.reshape(self.batch_size, -1)
            print(f"ğŸ“Œ Depois de reshape - perturbed_X.shape: {perturbed_X.shape}")

            assert perturbed_X.shape[1] == 12600, f"Formato incorreto: {perturbed_X.shape}"
            assert batch_X.shape[1] == 12600, f"Formato incorreto: {batch_X.shape}"
            
            print(f"ğŸ“Œ Shape antes do vmap: {perturbed_X.shape}")
            for i, x in enumerate(perturbed_X):
                print(f"ğŸš¨ Teste direto - Entrada {i} (shape): {x.shape}")
                if x.shape != (12600,):
                    print(f"âŒâŒâŒ ERRO: Entrada {i} tem shape invÃ¡lido: {x.shape} (esperado: (12600,)) âŒâŒâŒ")
                    exit(1)
            
            print(f"Perturbed_X.shape: {perturbed_X.shape}")
            print(f"Perturbed_X[0].shape: {perturbed_X[0].shape}")
            print(f"Params: {params}")

            # Aplicar act_func para cada entrada e empilhar as saÃ­das finais
            predictions_list = []
            for i, x in enumerate(perturbed_X):
                raw_output = act_func(state, x[None, :], params)
                print(f"ğŸš¨ Debug: Raw output for entrada {i}: {raw_output} (type: {type(raw_output)})")
                if isinstance(raw_output, (tuple, list)):
                    print(f"ğŸš¨ Debug: Length of raw output for entrada {i}: {len(raw_output)}")
                out = get_final_output(raw_output)
                predictions_list.append(out)
            
            predictions = jnp.stack(predictions_list)
            
            print(f"ğŸ“Œ Shape apÃ³s vmap: {predictions.shape}")
            
            batch_loss = -jnp.mean(jnp.sum(batch_y * jnp.log(predictions + 1e-9), axis=1))
            total_loss += batch_loss
        
        return -total_loss / num_batches

    @property
    def inputs(self):
        return self.data_inputs  # ğŸ”¹ Retorna os inputs (X_train)

    @property
    def targets(self):
        return self.data_outputs  # ğŸ”¹ Retorna os labels (y_train)

    @property
    def input_shape(self):
        """
        Retorna a forma esperada da entrada.
        O TensorNEAT espera um formato (features,).
        """
        return (self.data_inputs.shape[1],)  # Retorna o nÃºmero de features

    @property
    def output_shape(self):
        return self.data_outputs.shape  # ğŸ”¹ Retorna o shape dos labels (one-hot encoding)

def load_images_from_folder(folder_path, everything_at_once=False):
    """
    Carrega imagens do dataset em batches para evitar consumo excessivo de memÃ³ria.
    """
    data, labels = [], []
    
    for label in os.listdir(folder_path):
        label_path = os.path.join(folder_path, label)
        if os.path.isdir(label_path):
            images = os.listdir(label_path)
            num_images = int(len(images))
            for image_file in images[:num_images]:
                img_path = os.path.join(label_path, image_file)
                img = Image.open(img_path).convert('L')  # Escala de cinza
                img = img.resize((90, 140))  # Redimensiona
                img = (np.array(img) / 255.0).flatten()  # Normaliza
                
                data.append(img)
                labels.append(int(label))

                if not everything_at_once:
                    # ğŸ”¹ Se a memÃ³ria estiver alta, libere os dados periodicamente
                    if len(data) % 5000 == 0:
                        print(f"ğŸ”¹ Carregadas {len(data)} imagens, liberando memÃ³ria...")
                        yield np.array(data), np.array(labels)
                        data, labels = [], []

    if everything_at_once:
        # ğŸ”¹ Retorna todos os dados de uma sÃ³ vez
        yield np.array(data), np.array(labels)
    else:
        # ğŸ”¹ Retorna o restante dos dados
        yield np.array(data), np.array(labels)

def softmax(x):
    """FunÃ§Ã£o de ativaÃ§Ã£o Softmax usando JAX."""
    exp_x = jnp.exp(x - jnp.max(x))  # Para evitar overflow numÃ©rico
    return exp_x / jnp.sum(exp_x)

# Caminho para o dataset
path = r'/home/mileguir/.cache/kagglehub/datasets/olafkrastovski/handwritten-digits-0-9/versions/2'
if not os.path.exists(path):
    print(f"Dataset nÃ£o encontrado em {path}. Baixando...")
    path = kagglehub.dataset_download("olafkrastovski/handwritten-digits-0-9")
else:
    print(f"âœ… Dataset encontrado em {path}.")

# Carregar o dataset
X_data, y_data = [], []
for X_part, y_part in load_images_from_folder(path, everything_at_once=False):
    X_data.append(X_part)
    y_data.append(y_part)

# ğŸ”¹ Concatena os lotes de forma eficiente
X_data = np.concatenate(X_data, axis=0)
y_data = np.concatenate(y_data, axis=0)

# Converter labels para one-hot encoding
y_data = np.eye(10)[y_data]

# Dividir dataset em treino e teste (70% treino, 30% teste)
X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=42)
print(f"ğŸ”¹ Treino: {len(X_train)} amostras | Teste: {len(X_test)} amostras")

# Criar problema de aprendizado supervisionado
supervised_problem = SupervisedFuncFit(X_train, y_train, batch_size=32)

# Configura a arquitetura da rede neural
genome = DefaultGenome(
    num_inputs=12600,  # NÃºmero de pixels da imagem (entrada)
    num_outputs=10,  # 10 classes (0-9)
    max_nodes=13000, # NÃºmero mÃ¡ximo de neurÃ´nios
    max_conns = 250000, # NÃºmero mÃ¡ximo de conexÃµes
    mutation = DefaultMutation(), # MutaÃ§Ã£o padrÃ£o
    crossover = DefaultCrossover(), # Crossover padrÃ£o
    distance = DefaultDistance(), # DistÃ¢ncia padrÃ£o
    init_hidden_layers=(),  # Deixa o NEAT evoluir a estrutura oculta
    node_gene=BiasNode(
        activation_options=[ACT.sigmoid],  # AtivaÃ§Ã£o Sigmoid nos neurÃ´nios ocultos
        aggregation_options=[AGG.sum, AGG.product],  # OpÃ§Ãµes de agregaÃ§Ã£o
    ),
    conn_gene = DefaultConn(), # ConexÃ£o padrÃ£o
    output_transform=softmax,  # Softmax na saÃ­da para classificaÃ§Ã£o multiclasse
)

# Configura o algoritmo NEAT
algorithm = NEAT(
    pop_size=200,  # Tamanho da populaÃ§Ã£o
    species_size=20,  # NÃºmero de espÃ©cies na populaÃ§Ã£o
    survival_threshold=0.01,  # Percentual de sobrevivÃªncia por geraÃ§Ã£o
    genome=genome,  # Usa o genoma configurado
)

# Configurar e rodar o pipeline NEAT
pipeline = Pipeline(
    algorithm=algorithm,
    problem=supervised_problem,
    generation_limit=50,
    fitness_target=-0.01,
    seed=42,
)

# Inicializar o estado do NEAT
state = pipeline.setup()

# Executar evoluÃ§Ã£o
state, best = pipeline.auto_run(state)

# Testar a melhor rede neural no conjunto de teste
best_network = best.make_network()
test_predictions = np.array([best_network(x) for x in X_test])

# Calcular acurÃ¡cia
test_accuracy = np.mean(np.argmax(test_predictions, axis=1) == np.argmax(y_test, axis=1))
print(f"ğŸ¯ AcurÃ¡cia final no conjunto de teste: {test_accuracy * 100:.2f}%")

# Salvar modelo treinado
with open("best_neat_model.pkl", "wb") as f:
    pickle.dump(best, f)
print("âœ… Modelo salvo como 'best_neat_model.pkl'")
