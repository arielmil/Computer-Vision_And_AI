import os
import kagglehub
import numpy as np
import pickle
from PIL import Image
from sklearn.model_selection import train_test_split
from tensorneat.problem.supervised import SupervisedFuncFit
from tensorneat.pipeline import Pipeline
from tensorneat.algorithm import NEAT
from tensorneat.algorithm import NEAT
from tensorneat.genome import DefaultGenome
from tensorneat.common import ACT, AGG
from tensorneat.genome.gene.node.bias import BiasNode
from typing import Union, List, Tuple
from jax import numpy as jnp, vmap
import jax.numpy as jnp
import numpy as np
from tensorneat.problem.func_fit import FuncFit

class SupervisedFuncFit(FuncFit):
    def __init__(
        self,
        X: Union[List, Tuple, np.ndarray],
        y: Union[List, Tuple, np.ndarray],
        batch_size: int = 256,  # üîπ Processamento em lotes para reduzir consumo de mem√≥ria
        *args,
        **kwargs,
    ):
        """
        Problema de aprendizado supervisionado para TensorNEAT.

        X: Features (inputs)
        y: Labels (outputs, one-hot encoded)
        batch_size: Tamanho do batch para avalia√ß√£o (evita estouro de mem√≥ria)
        """
        self.data_inputs = jnp.array(X, dtype=jnp.float32)
        self.data_outputs = jnp.array(y, dtype=jnp.float32)
        self.batch_size = batch_size

        super().__init__(*args, **kwargs)

    def evaluate(self, state, randkey, act_func, params):
        """
        Calcula a fun√ß√£o de fitness usando Cross-Entropy Loss em batches.
        """
        num_samples = self.data_inputs.shape[0]
        total_loss = 0.0

        for i in range(0, num_samples, self.batch_size):
            batch_X = self.data_inputs[i : i + self.batch_size]
            batch_y = self.data_outputs[i : i + self.batch_size]

            # üîπ Usa `act_func` corretamente dentro do NEAT para prever as sa√≠das
            predictions = vmap(lambda x: act_func(state, x, params))(batch_X)

            # üîπ Calcula Cross-Entropy Loss no batch
            batch_loss = -jnp.mean(jnp.sum(batch_y * jnp.log(predictions + 1e-9), axis=1))
            total_loss += batch_loss

        return -total_loss / (num_samples / self.batch_size)  # üîπ Fitness = -Loss

    @property
    def inputs(self):
        return self.data_inputs  # üîπ Retorna os inputs (X_train)

    @property
    def targets(self):
        return self.data_outputs  # üîπ Retorna os labels (y_train)

    @property
    def input_shape(self):
        return self.data_inputs.shape  # üîπ Retorna o shape (n_amostras, n_features)

    @property
    def output_shape(self):
        return self.data_outputs.shape  # üîπ Retorna o shape dos labels (one-hot encoding)

def load_images_from_folder(folder_path, use_percentage=1.0):
    """
    Carrega imagens do dataset, normaliza e converte para um formato adequado para TensorNEAT.
    use_percentage: Determina a porcentagem das imagens a serem usadas.
    """
    data, labels = [], []
    
    for label in os.listdir(folder_path):
        label_path = os.path.join(folder_path, label)
        if os.path.isdir(label_path):
            images = os.listdir(label_path)
            num_images = int(len(images) * use_percentage)
            for image_file in images[:num_images]:
                img_path = os.path.join(label_path, image_file)
                img = Image.open(img_path).convert('L')  # Converter para escala de cinza
                img = img.resize((90, 140))  # Ajustar para o tamanho correto (90x140)
                img = (np.array(img) / 255.0).flatten()  # Normalizar e achatar
                data.append(img)
                labels.append(int(label))
    
    return np.array(data), np.array(labels)


def softmax(x):
    """Fun√ß√£o de ativa√ß√£o Softmax usando JAX."""
    exp_x = jnp.exp(x - jnp.max(x))  # Para evitar overflow num√©rico
    return exp_x / jnp.sum(exp_x)

# Caminho para o dataset
path = r'C:/Users/mileguir/.cache/kagglehub/datasets/olafkrastovski/handwritten-digits-0-9/versions/2'
if not os.path.exists(path):
    print(f"Dataset n√£o encontrado em {path}. Baixando...")
    path = kagglehub.dataset_download("olafkrastovski/handwritten-digits-0-9")
else:
    print(f"‚úÖ Dataset encontrado em {path}.")

# Carregar o dataset
X_data, y_data = load_images_from_folder(path)
print(f"‚úÖ Dataset carregado com {len(X_data)} amostras!")

# Converter labels para one-hot encoding
y_data = np.eye(10)[y_data]

# Dividir dataset em treino e teste (70% treino, 30% teste)
X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=42)
print(f"üîπ Treino: {len(X_train)} amostras | Teste: {len(X_test)} amostras")

# Criar problema de aprendizado supervisionado
supervised_problem = SupervisedFuncFit(X_train, y_train, batch_size=128)

# Configurar a arquitetura da rede neural
genome = DefaultGenome(
    num_inputs=12600,  # N√∫mero de pixels da imagem (entrada)
    num_outputs=10,  # 10 classes (0-9)
    max_nodes=13000, # N√∫mero m√°ximo de neur√¥nios
    max_conns = 250000, # N√∫mero m√°ximo de conex√µes
    init_hidden_layers=(),  # Deixa o NEAT evoluir a estrutura oculta
    node_gene=BiasNode(
        activation_options=[ACT.sigmoid],  # Ativa√ß√£o Sigmoid nos neur√¥nios ocultos
        aggregation_options=[AGG.sum, AGG.product],  # Op√ß√µes de agrega√ß√£o
    ),
    output_transform=softmax,  # Softmax na sa√≠da para classifica√ß√£o multiclasse
)

# Configurar o algoritmo NEAT
algorithm = NEAT(
    pop_size=200,  # Tamanho da popula√ß√£o
    species_size=20,  # N√∫mero de esp√©cies na popula√ß√£o
    survival_threshold=0.01,  # Percentual de sobreviv√™ncia por gera√ß√£o
    genome=genome,  # Usa o genoma configurado
)

# Configurar e rodar o pipeline NEAT
pipeline = Pipeline(
    algorithm=algorithm,
    problem=supervised_problem,
    generation_limit=50,
    fitness_target=-0.01,
    seed=42,
)

# Inicializar o estado do NEAT
state = pipeline.setup()

# Executar evolu√ß√£o
state, best = pipeline.auto_run(state)

# Testar a melhor rede neural no conjunto de teste
best_network = best.make_network()
test_predictions = np.array([best_network(x) for x in X_test])

# Calcular acur√°cia
test_accuracy = np.mean(np.argmax(test_predictions, axis=1) == np.argmax(y_test, axis=1))
print(f"üéØ Acur√°cia final no conjunto de teste: {test_accuracy * 100:.2f}%")

# Salvar modelo treinado
with open("best_neat_model.pkl", "wb") as f:
    pickle.dump(best, f)
print("‚úÖ Modelo salvo como 'best_neat_model.pkl'")
